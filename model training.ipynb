{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44fd7571",
   "metadata": {},
   "source": [
    "                                                18S01ACS011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851902b",
   "metadata": {},
   "source": [
    "First step is to import all the necessary libraries for the dataset loading and the model teaining procedure.\n",
    "\n",
    "In this notebook I will take use of the Encoder-Decoder architecture for NMT due to its flexibility and ease to train a single end to end model directly on source and target sentences and its ability to handle variable input and output sequences of text which my dataset is based on.\n",
    "\n",
    "The model 'WarwinuTranslates' has been trained on languge pairs of english sentences as the domain language and corresponding gikuyu sentences to help improve on the quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "bf2411b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "e17c5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "\n",
    "myBatchSize=28\n",
    "nduati_epochs=30\n",
    "latent_dim=100\n",
    "samples=1350\n",
    "\n",
    "data_path = \"C:/Users/user/Desktop/clean.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d33e0",
   "metadata": {},
   "source": [
    "The next step is vectorizing the data. Vectorizing is a technique in machine learning to make code to execute fast by optimization of algorithms.\n",
    "\n",
    "it is the process of converting input data from it's raw format to real numbers since the computer only understands the binary language of zero's and one's.It is specifically useful in machine learning for feature extraction where some distinct features are acquired from the text for the model to train on through conversion of text to numeric vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "c67fc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "nduatiInputTxts = []\n",
    "nduatiTargetTxts = []\n",
    "nduatiInputChars = set()\n",
    "nduatiTargetChars = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(samples, len(lines) - 1)]:\n",
    "    unpack = re.split(r'\\t+', line)\n",
    "    \n",
    "    #input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    nduatiInputTxt, nduatiTargetTxt = unpack\n",
    "    \n",
    "    # I use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    \n",
    "    nduatiTargetTxt = '\\t' + nduatiTargetTxt + '\\n'\n",
    "    nduatiInputTxts.append(nduatiInputTxt)\n",
    "    nduatiTargetTxts.append(nduatiTargetTxt)\n",
    "    for val in nduatiInputTxt:\n",
    "        if val not in nduatiInputChars:\n",
    "            nduatiInputChars.add(val)\n",
    "    for val in nduatiTargetTxt:\n",
    "        if val not in nduatiTargetChars:\n",
    "            nduatiTargetChars.add(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "2343f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "nduatiInputChars=sorted(list(nduatiInputChars))\n",
    "nduatiTargetChars=sorted(list(nduatiTargetChars))\n",
    "\n",
    "my_encoder_tokens=len(nduatiInputChars)\n",
    "my_decoder_tokens=len(nduatiTargetChars)\n",
    "\n",
    "maxSeqEncoderLength=max([len(txt) for txt in nduatiInputTxts])\n",
    "maxSeqDecoderLength=max([len(txt) for txt in nduatiTargetTxts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "914992c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nduati dataset samples: 1348\n",
      "Unique input tokens: 43\n",
      "Unique output tokens: 44\n",
      "Max sequence length for inputs: 481\n",
      "Max sequence length for outputs: 543\n"
     ]
    }
   ],
   "source": [
    "print('Nduati dataset samples:', len(nduatiInputTxts))\n",
    "print('Unique input tokens:', my_encoder_tokens)\n",
    "print('Unique output tokens:', my_decoder_tokens)\n",
    "print('Max sequence length for inputs:', maxSeqEncoderLength)\n",
    "print('Max sequence length for outputs:', maxSeqDecoderLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "1437219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nduatiInputTokenIndex=dict(\n",
    "    [(val,i) for i, val in enumerate(nduatiInputChars)])\n",
    "nduatiTargetTokenIndex=dict(\n",
    "[(val,i) for i, val in enumerate(nduatiTargetChars)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "f53dcb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "nduatiEncoderInputData = np.zeros(\n",
    "    (len(nduatiInputTxts), maxSeqEncoderLength, my_encoder_tokens),\n",
    "    dtype='float32')\n",
    "nduatiDecoderInputData = np.zeros(\n",
    "    (len(nduatiInputTxts), maxSeqDecoderLength, my_decoder_tokens),\n",
    "    dtype='float32')\n",
    "nduatiDecoderTargetData = np.zeros(\n",
    "    (len(nduatiInputTxts), maxSeqDecoderLength, my_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "7822acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (nduatiInputTxt, nduatiTargetTxt) in enumerate(zip(nduatiInputTxts, nduatiTargetTxts)):\n",
    "    for t, val in enumerate(nduatiInputTxt):\n",
    "        nduatiEncoderInputData[i,t, nduatiInputTokenIndex[val]] = 1.\n",
    "    nduatiEncoderInputData[i, t + 1:, nduatiInputTokenIndex[' ']] = 1.\n",
    "    for t, val in enumerate(nduatiTargetTxt):\n",
    "        \n",
    "        # nduatiDecoderTargetData is ahead of decoder_input_data by one timestep\n",
    "        \n",
    "        nduatiDecoderInputData[i, t, nduatiTargetTokenIndex[val]] = 1.\n",
    "        if t > 0:\n",
    "            # nduatiDecoderTargetData will be ahead by one timestep\n",
    "            \n",
    "            # and will not include the start character.\n",
    "            nduatiDecoderTargetData[i, t - 1, nduatiTargetTokenIndex[val]] = 1.\n",
    "    nduatiDecoderInputData[i, t + 1:, nduatiTargetTokenIndex[' ']] = 1.\n",
    "    nduatiDecoderTargetData[i, t:, nduatiTargetTokenIndex[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "5fa881c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "encoderModel = Model(nduatiEncoderInputs, myEncoderStates)\n",
    "\n",
    "decoderStateInput_h = Input(shape=(latent_dim,))\n",
    "decoderStateInput_c = Input(shape=(latent_dim,))\n",
    "decoderStateInputs = [decoderStateInput_h, decoderStateInput_c]\n",
    "myDecoderOutputs, state_h, state_c = myDecoderLstm(\n",
    "    nduatiDecoderInputs, initial_state=decoderStateInputs)\n",
    "decoderStates = [state_h, state_c]\n",
    "myDecoderOutputs = decoderDense(myDecoderOutputs)\n",
    "decoderModel = Model(\n",
    "    [nduatiDecoderInputs] + decoderStateInputs,\n",
    "    [myDecoderOutputs] + decoderStates)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverseInputCharIndex = dict(\n",
    "    (i, char) for char, i in nduatiInputTokenIndex.items())\n",
    "reverseTargetCharIndex = dict(\n",
    "    (i, char) for char, i in nduatiTargetTokenIndex.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b07777",
   "metadata": {},
   "source": [
    "The next important step is defining an encoder and a decoder and using it.In this notebook I have used the encoder-decoder model which is a way of using recurrent neural networks for sequence-to-sequence prediction problems.\n",
    "\n",
    "\n",
    "The encoder encodes the input sequence and the decoder decodes the encoded input sequence into the target sequence.The sequence to sequence model consists of three parts.\n",
    "\n",
    "1. The encoder which accepts a single element of the input sequence at each time step,process it,collects nformation for that element and propagates it forward\n",
    "2. The Intermediate vector which is the final internal state produced from the encoder part of the model.Information on the entire input sequence to help the decoder make accurate predictions is contained here.\n",
    "3. Finally we have the decoder which when given an entire sentence it predicts an output at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "90a94c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an input sequence and processing it .\n",
    "\n",
    "nduatiEncoderInputs = Input(shape=(None, my_encoder_tokens))\n",
    "myEncoder = LSTM(latent_dim, return_state=True)\n",
    "myEncoderOutputs, state_h, state_c = myEncoder(nduatiEncoderInputs)\n",
    "# We discard myEncoderOutputs and only keep the states only.\n",
    "\n",
    "myEncoderStates = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using myEncoderStates as the  initial state.\n",
    "\n",
    "nduatiDecoderInputs = Input(shape=(None, my_decoder_tokens))\n",
    "# Then we set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. \n",
    "#We don't use the return states in the training model, but we will use them in inference.\n",
    "\n",
    "myDecoderLstm = LSTM(latent_dim,return_sequences=True, return_state=True)\n",
    "myDecoderOutputs, _, _ = myDecoderLstm(nduatiDecoderInputs,\n",
    "                                     initial_state=myEncoderStates)\n",
    "decoderDense = Dense(my_decoder_tokens, activation='softmax')\n",
    "myDecoderOutputs = decoderDense(myDecoderOutputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e263804",
   "metadata": {},
   "source": [
    "The next important thing I will use is EArly Stopping which works in reducing overfitting and underfitting in training the model thus increasing the accuracy in predicting data that was not initially in the training set.\n",
    "\n",
    "The early stopping will stop the training of the model once the model validation accuracy has stopped improving to prevent the model from mastering all the data in the training set and lowering its performance in a new problem which was not inluded in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "d93aeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "myEarlyStopping = EarlyStopping(monitor='val_accuracy' , patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427a1d2",
   "metadata": {},
   "source": [
    "Next preocess is the machine training which will use the RNN_LSTM Algorithm.\n",
    "\n",
    "Recurrent Neural Networks (RNN) takes a sequence of text as inputs or returns sequences of texts as output or both.The RNN network hidden layer hsas a loop in which the output and cell state from each time step becomes an input of the next time step and the recurrence serves as a form of memory.\n",
    "\n",
    "Long-Short-Term-Memory (LSTM) is a modofied version of RNN that makes it easier to remember pAST data in the memory solving the memory dependency problem thus improving the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "474aa386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "39/39 [==============================] - 297s 4s/step - loss: 1.4937 - accuracy: 0.8415 - val_loss: 0.4459 - val_accuracy: 0.9318\n",
      "Epoch 2/30\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.3754 - accuracy: 0.9382 - val_loss: 0.3701 - val_accuracy: 0.9329\n",
      "Epoch 3/30\n",
      "39/39 [==============================] - 75s 2s/step - loss: 0.3371 - accuracy: 0.9387 - val_loss: 0.3258 - val_accuracy: 0.9331\n",
      "Epoch 4/30\n",
      "39/39 [==============================] - 71s 2s/step - loss: 0.2655 - accuracy: 0.9391 - val_loss: 0.2454 - val_accuracy: 0.9331\n",
      "Epoch 5/30\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.2148 - accuracy: 0.9391 - val_loss: 0.2297 - val_accuracy: 0.9339\n",
      "Epoch 6/30\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.2064 - accuracy: 0.9398 - val_loss: 0.2247 - val_accuracy: 0.9348\n",
      "Epoch 7/30\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.2029 - accuracy: 0.9411 - val_loss: 0.2218 - val_accuracy: 0.9369\n",
      "Epoch 8/30\n",
      "39/39 [==============================] - 70s 2s/step - loss: 0.2000 - accuracy: 0.9425 - val_loss: 0.2189 - val_accuracy: 0.9385\n",
      "Epoch 9/30\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.1971 - accuracy: 0.9442 - val_loss: 0.2156 - val_accuracy: 0.9386\n",
      "Epoch 10/30\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.1938 - accuracy: 0.9455 - val_loss: 0.2120 - val_accuracy: 0.9415\n",
      "Epoch 11/30\n",
      "39/39 [==============================] - 68s 2s/step - loss: 0.1901 - accuracy: 0.9467 - val_loss: 0.2076 - val_accuracy: 0.9423\n",
      "Epoch 12/30\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.1859 - accuracy: 0.9474 - val_loss: 0.2033 - val_accuracy: 0.9421\n",
      "Epoch 13/30\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.1818 - accuracy: 0.9476 - val_loss: 0.1992 - val_accuracy: 0.9427\n",
      "Epoch 14/30\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.1781 - accuracy: 0.9478 - val_loss: 0.1954 - val_accuracy: 0.9425\n",
      "Epoch 15/30\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.1748 - accuracy: 0.9480 - val_loss: 0.1919 - val_accuracy: 0.9426\n",
      "Epoch 16/30\n",
      "39/39 [==============================] - 66s 2s/step - loss: 0.1719 - accuracy: 0.9480 - val_loss: 0.1890 - val_accuracy: 0.9429\n",
      "Epoch 17/30\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.1692 - accuracy: 0.9481 - val_loss: 0.1864 - val_accuracy: 0.9429\n",
      "Epoch 18/30\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.1669 - accuracy: 0.9482 - val_loss: 0.1841 - val_accuracy: 0.9432\n",
      "Epoch 19/30\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.1650 - accuracy: 0.9485 - val_loss: 0.1827 - val_accuracy: 0.9436\n",
      "Epoch 20/30\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.1633 - accuracy: 0.9488 - val_loss: 0.1805 - val_accuracy: 0.9438\n",
      "Epoch 21/30\n",
      "39/39 [==============================] - 68s 2s/step - loss: 0.1617 - accuracy: 0.9493 - val_loss: 0.1789 - val_accuracy: 0.9447\n",
      "Epoch 22/30\n",
      "39/39 [==============================] - 70s 2s/step - loss: 0.1603 - accuracy: 0.9496 - val_loss: 0.1777 - val_accuracy: 0.9450\n",
      "Epoch 23/30\n",
      "39/39 [==============================] - 67s 2s/step - loss: 0.1591 - accuracy: 0.9499 - val_loss: 0.1763 - val_accuracy: 0.9450\n",
      "Epoch 24/30\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.1580 - accuracy: 0.9501 - val_loss: 0.1754 - val_accuracy: 0.9451\n",
      "Epoch 25/30\n",
      "39/39 [==============================] - 71s 2s/step - loss: 0.1571 - accuracy: 0.9503 - val_loss: 0.1745 - val_accuracy: 0.9458\n",
      "Epoch 26/30\n",
      "39/39 [==============================] - 70s 2s/step - loss: 0.1563 - accuracy: 0.9504 - val_loss: 0.1737 - val_accuracy: 0.9454\n",
      "Epoch 27/30\n",
      "39/39 [==============================] - 64s 2s/step - loss: 0.1555 - accuracy: 0.9507 - val_loss: 0.1732 - val_accuracy: 0.9459\n",
      "Epoch 28/30\n",
      "39/39 [==============================] - 68s 2s/step - loss: 0.1549 - accuracy: 0.9507 - val_loss: 0.1722 - val_accuracy: 0.9457\n",
      "Epoch 29/30\n",
      "39/39 [==============================] - 70s 2s/step - loss: 0.1541 - accuracy: 0.9509 - val_loss: 0.1715 - val_accuracy: 0.9460\n",
      "Epoch 30/30\n",
      "39/39 [==============================] - 80s 2s/step - loss: 0.1535 - accuracy: 0.9510 - val_loss: 0.1714 - val_accuracy: 0.9458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x235aa40c760>"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# nduatiEncoderInputData  & nduatiDecoderInputData into nduatiDecoderTArgetData\n",
    "model = Model([nduatiEncoderInputs, nduatiDecoderInputs], myDecoderOutputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([nduatiEncoderInputData, nduatiDecoderInputData], nduatiDecoderTargetData,\n",
    "          batch_size=myBatchSize,\n",
    "          epochs=nduati_epochs,\n",
    "          validation_split=0.2,\n",
    "         callbacks=[myEarlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "e74d1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('WarwinuTranslates.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a988f",
   "metadata": {},
   "source": [
    "Next step is to use the model to decode an english sentence input to a kikuyu sentence output using a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "b2f13379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def nduatiDecodeSequence(myInputSeq):\n",
    "    \n",
    "    # Encode inputs as state vectors.\n",
    "    nduatiStatesValue = encoderModel.predict(myInputSeq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    myTargetSeq = np.zeros((1, 1, my_decoder_tokens))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    myTargetSeq[0, 0, nduatiTargetTokenIndex['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
    "    \n",
    "    stopCondition = False\n",
    "    decodedSentence = ''\n",
    "    while not stopCondition:\n",
    "        outputTokens, h, c = decoderModel.predict(\n",
    "            [myTargetSeq] + nduatiStatesValue)\n",
    "        \n",
    "         # Sample a token and pick one with highest probability\n",
    "            \n",
    "        sampledTokenIndex = np.argmax(outputTokens[0, -1, :])\n",
    "        sampledChar = reverseTargetCharIndex[sampledTokenIndex]\n",
    "        decodedSentence += sampledChar\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        \n",
    "        if (sampledChar == '\\n' or\n",
    "           len(decodedSentence) > maxSeqDecoderLength):\n",
    "            stopCondition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        targetSeq = np.zeros((1, 1, my_decoder_tokens))\n",
    "        targetSeq[0, 0, sampledTokenIndex] = 1.\n",
    "\n",
    "        # Update states\n",
    "        nduatiStatesValue = [h, c]\n",
    "\n",
    "    return decodedSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "4784d0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x00000235AA7023A0>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "8db854ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nduatiTranslator:\n",
    "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "  \n",
    "  #Method to start the translator\n",
    "  def start(self):\n",
    "    user_response = input(\"Input an English sentence. :) \\n\")\n",
    "    self.translate(user_response)\n",
    "  \n",
    "  #Method to handle the conversation\n",
    "  def translate(self, reply):\n",
    "    while not self.make_exit(reply):\n",
    "      reply = input(self.generate_response(reply)+\"\\n\")\n",
    "\n",
    "  #Method to convert user input into a matrix\n",
    "  def string_to_matrix(self, user_input):\n",
    "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
    "    user_input_matrix = np.zeros(\n",
    "      (1, maxSeqEncoderLength, my_encoder_tokens),\n",
    "      dtype='float32')\n",
    "    for timestep, token in enumerate(tokens):\n",
    "      if token in nduatiInputTokenIndex:\n",
    "        user_input_matrix[0, timestep, nduatiInputTokenIndex[token]] = 1.\n",
    "    return user_input_matrix\n",
    "  \n",
    "  #Method that will create a response using seq2seq model we built\n",
    "  def generate_response(self, user_input):\n",
    "    input_matrix = self.string_to_matrix(user_input)\n",
    "    chatbot_response = decode_response(input_matrix)\n",
    "    #Remove  and  tokens from chatbot_response\n",
    "    chatbot_response = chatbot_response.replace(\"\",'')\n",
    "    chatbot_response = chatbot_response.replace(\"\",'')\n",
    "    return chatbot_response\n",
    "\n",
    " #Method to check for exit commands\n",
    "  def make_exit(self, reply):\n",
    "    for exit_command in self.exit_commands:\n",
    "      if exit_command in reply:\n",
    "        print(\"Have a Great Day!\")\n",
    "        return True\n",
    "    return False\n",
    "  \n",
    "WarwinuTranslates = nduatiTranslator()\n",
    "  \n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249dd157",
   "metadata": {},
   "source": [
    "Next step I will build  a translator which will provide a kind of 'user interface' for input of an english statement and provides the corresponding kikuyu output.The translator will also have keywords which the user can type to terminate the translator program from running including words like :-\n",
    "1. Quit                    \n",
    "2. Pause                   \n",
    "3. Exit                    \n",
    "4. Goodbye\n",
    "5. Bye\n",
    "6. Later\n",
    "6. Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "c2821c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input an English sentence. :) \n",
      "person\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 1s 766ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 355ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      " * — + + + + x x x ‘ ‘ f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’ + + + + x x x x ‘ ‘ f f f f ’ ’\n",
      "exit\n",
      "Have a Great Day!\n"
     ]
    }
   ],
   "source": [
    "WarwinuTranslates.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8493222",
   "metadata": {},
   "source": [
    "The model training has been successfull but the translation is where it's not behaving as expecte.I'm currently working on extending the dataset to evaluate if performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fde2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
